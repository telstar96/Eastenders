{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Improve pre-processing (20 marks)\n",
    "Using the pre-processing techniques you have learned in the module, improve the `pre_process` function above, which currently just tokenizes text based on white space.\n",
    "\n",
    "When developing, use the 90% train and 10% validation data split from the training file, using the first 360 lines from the training split and first 40 lines from the validation split, as per above. To check the improvements by using the different techniques, use the `compute_IR_evaluation_scores` function as above. The **mean rank** is the main metric you need to focus on improving throughout this assignment, where the target/best possible performance is **1** (i.e. all test/validation data character documents are closest to their corresponding training data character documents) and the worst is **16**. Initially the code in this template achieves a mean rank of **5.12**  and accuracy of **0.3125** on the test set and a mean rank of **4.5** and accuracy of - you should be looking to improve those, particularly getting the mean rank as close to 1 as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline mean rank (tokenize on whitespace): 4.5, accuracy: 0.25\n",
    "\n",
    "# Normalization\n",
    "# tokens = [token.lower() for token in tokens]\n",
    "# Mean rank: 4.875, accuracy: 0.375\n",
    "\n",
    "# Stop words\n",
    "# tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "# Mean rank: 3.4375, accuracy: 0.5\n",
    "\n",
    "# Lemmatization\n",
    "# tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "# Mean rank: 4.5, accuracy: 0.3125\n",
    "\n",
    "# Porter Stemmer (without lowercasing)\n",
    "# tokens = [porter.stem(token, for_lowercase=False) for token in tokens]\n",
    "# Mean rank: 4.5, accuracy: 0.3125\n",
    "# Porter Stemmer (with lowercasing)\n",
    "# tokens = [porter.stem(token, for_lowercase=True) for token in tokens]\n",
    "# Mean rank: 5.0, accuracy: 0.375\n",
    "\n",
    "\n",
    "\n",
    "# Several methods\n",
    "# split on white space\n",
    "# tokens = re.split('\\W+', character_text)\n",
    "# # remove empty tokens\n",
    "# tokens = [t for t in tokens if t]\n",
    "# # convert to lower case\n",
    "# tokens = [t.lower() for t in tokens]\n",
    "# # remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tokens = [t for t in tokens if t not in stop_words]\n",
    "# # remove numbers\n",
    "# tokens = [t for t in tokens if not t.isdigit()]\n",
    "# # remove single character tokens\n",
    "# tokens = [t for t in tokens if len(t)>1]\n",
    "# # remove tokens with non-alphabetic characters\n",
    "# tokens = [t for t in tokens if t.isalpha()]\n",
    "# # stem tokens\n",
    "# porter = PorterStemmer()\n",
    "# tokens = [porter.stem(t) for t in tokens]\n",
    "# Mean rank: 2.0, accuracy: 0.6875\n",
    "\n",
    "# Num to word\n",
    "# # tokenize on white space\n",
    "# tokens = character_text.split()\n",
    "# # convert numbers to words (e.g. 1 -> one)\n",
    "# tokens = [num2words(int(t)) if t.isdigit() else t for t in tokens]\n",
    "\n",
    "# Best so far\n",
    "# returns mean rank of 1.8125, accuracy: 0.6875\n",
    "def pre_process(character_text):\n",
    "    \"\"\"Pre-process all the concatenated lines of a character, \n",
    "    using tokenization, spelling normalization and other techniques.\n",
    "    \n",
    "    Initially just a tokenization on white space. Improve this for Q1.\n",
    "    \n",
    "    ::character_text:: a string with all of one character's lines\n",
    "    \"\"\"\n",
    "    # split on white space \n",
    "    tokens = re.split('\\W+', character_text)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # convert numbers to words (e.g. 1 -> one)\n",
    "    tokens = [num2words(int(t)) if t.isdigit() else t for t in tokens]\n",
    "\n",
    "    # remove tokens with non-alphabetic characters\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "    # lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # stem tokens\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(t) for t in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Improve linguistic feature extraction (20 marks)\n",
    "Use the feature extraction techniques you have learned to improve the `to_feature_vector_dictionary` function above. Examples of extra features could include extracting n-grams of different lengths and including POS-tags. You could also use sentiment analysis or another text classifier's result when applied to the features for each character document. You could even use a gender classifier trained on the same data using the GENDER column **(but DO NOT USE the GENDER column directly in the features for the final vector)**.\n",
    "\n",
    "You could use feature selection/reduction with techniques like minimum/maximum document frequency and/or feature selection like k-best selection using different statistical tests https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html. Again, develop on 90% training and 10% validation split and note the effect/improvement in mean rank with the techniques you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline = 1.8125\n",
    "\n",
    "# # replace the counts with the log of the counts \n",
    "#     for key in counts.keys():\n",
    "#         counts[key] = np.log(counts[key])\n",
    "#     return counts\n",
    "# Mean rank: 1.625\n",
    "\n",
    "# # tf-idf model\n",
    "#     counts = Counter(character_doc)\n",
    "#     counts = dict(counts)\n",
    "#     tf = {k: v/len(character_doc) for k, v in counts.items()}\n",
    "#     idf = {k: math.log(len(character_doc)/v) for k, v in counts.items()}\n",
    "#     tf_idf = {k: v*idf[k] for k, v in tf.items()}\n",
    "#     counts = tf_idf\n",
    "#     return counts\n",
    "# Mean rank: 1.5\n",
    "\n",
    "# # tf-idf model with bigrams\n",
    "# # extract n-grams\n",
    "#     n_grams = []\n",
    "#     for i in range(1, 3):\n",
    "#         n_grams += list(ngrams(character_doc, i))\n",
    "#     counts = Counter(n_grams)  # for now a simple count\n",
    "#     counts = dict(counts)\n",
    "#     # tf-idf\n",
    "#     tf = {k: v/len(character_doc) for k, v in counts.items()}\n",
    "#     idf = {k: math.log(len(character_doc)/v) for k, v in counts.items()}\n",
    "#     tf_idf = {k: v*idf[k] for k, v in tf.items()}\n",
    "#     counts = tf_idf\n",
    "#     return counts\n",
    "# Mean rank: 1.3125\n",
    "\n",
    "\n",
    "# # extract n-grams\n",
    "#     n_grams = []\n",
    "#     for i in range(1, 4):\n",
    "#         n_grams += list(ngrams(character_doc, i))\n",
    "#     counts = Counter(n_grams)  # for now a simple count\n",
    "#     counts = dict(counts)\n",
    "#     # remove low frequency n-grams\n",
    "#     for key in list(counts.keys()):\n",
    "#         if counts[key] < 2:\n",
    "#             del counts[key]\n",
    "#     return counts\n",
    "# Mean rank: 2.75, accuracy: 0.5\n",
    "\n",
    "# # extract n-grams\n",
    "#     n_grams = []\n",
    "#     for i in range(1, 4):\n",
    "#         n_grams += list(ngrams(character_doc, i))\n",
    "#     counts = Counter(n_grams)  # for now a simple count\n",
    "#     counts = dict(counts)\n",
    "#     return counts\n",
    "# Mean rank: 2.0, accuracy: 0.6875\n",
    "\n",
    "#  # pos tags\n",
    "#     pos_tags = nltk.pos_tag(character_doc)\n",
    "#     pos_tags = Counter(pos_tags)\n",
    "#     pos_tags = dict(pos_tags)\n",
    "    \n",
    "#     return pos_tags\n",
    "# Mean rank: 2.0, accuracy: 0.625\n",
    "\n",
    "# Adding pos tags and sentiment scores\n",
    "# # pos tags\n",
    "#     pos_tags = nltk.pos_tag(character_doc)\n",
    "#     pos_tags = Counter(pos_tags)\n",
    "#     pos_tags = dict(pos_tags)\n",
    "#     # add sentiment scores to each pos_tag\n",
    "#     sentiment_scores = []\n",
    "#     for word in character_doc:\n",
    "#         sentiment_scores.append(TextBlob(word).sentiment.polarity)\n",
    "#     sentiment_scores = Counter(sentiment_scores)\n",
    "#     sentiment_scores = dict(sentiment_scores)\n",
    "#     # add sentiment_score keys to pos_tags\n",
    "#     for key in sentiment_scores.keys():\n",
    "#         pos_tags[key] = sentiment_scores[key]\n",
    "#     return pos_tags\n",
    "# Mean rank: 1.8125\n",
    "\n",
    "# Extracting bigrams and unigrams\n",
    "# # extract bigrams and unigrams\n",
    "#     unigrams = Counter(character_doc)\n",
    "#     unigrams = dict(unigrams)\n",
    "#     # extract bigrams\n",
    "#     bigrams = list(ngrams(character_doc, 2))\n",
    "#     bigrams = [b[0] + '_' + b[1] for b in bigrams]\n",
    "#     bigrams = Counter(bigrams)\n",
    "#     bigrams = dict(bigrams)\n",
    "#     # make unigrams into a Counter\n",
    "#     counts = Counter(unigrams)\n",
    "#     counts = dict(counts)\n",
    "#     # remove underscores from bigrams\n",
    "#     bigrams = {k.replace('_', ' '): v for k, v in bigrams.items()}\n",
    "#     # combine unigrams and bigrams\n",
    "#     counts.update(bigrams)\n",
    "#     return counts\n",
    "# Mean Rank: 1.875, accuracy: 0.75\n",
    "\n",
    "# Combine pos tags and bigrams\n",
    "# # extract pos tags\n",
    "#     pos_tags = nltk.pos_tag(character_doc)\n",
    "#     pos_tags = Counter(pos_tags)\n",
    "#     pos_tags = dict(pos_tags)\n",
    "#     # extract bigrams and add pos tags to them\n",
    "#     bigrams = list(ngrams(character_doc, 2))\n",
    "#     bigrams = [b[0] + '_' + b[1] for b in bigrams]\n",
    "#     bigrams = Counter(bigrams)\n",
    "#     bigrams = dict(bigrams)\n",
    "#     bigrams = nltk.pos_tag(bigrams)\n",
    "#     bigrams = Counter(bigrams)\n",
    "#     bigrams = dict(bigrams)\n",
    "#     # Update pos_tags dictionary with bigrams\n",
    "#     pos_tags.update(bigrams)\n",
    "#     return pos_tags\n",
    "# Mean rank: 2.0625, accuracy: 0.5625\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Analyse the similarity results (10 marks)\n",
    "From your system so far run on the 90%/10% training/validation split, identify the heldout character vectors ranked closest to each character's training vector which are not the character themselves, and those furthest away, as displayed using the `plot_heat_map_similarity` function. In your report, try to ascribe reasons why this is the case, particularly for those where there isn't a successful highest match between the target character in the training set and that character's vector in the heldout set yet. Observations you could make include how their language use is similar, resulting in similar word or ngram features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christian is most similar to Jane and least similar to Shirley\n",
    "Clare most similar to Max and least similar to Ronnie\n",
    "Heather most similar to Phil and least similar to Shirley\n",
    "Ian most similar to Max and least similar to Ronnie\n",
    "Jack most similar to Max and least similar to Shirley\n",
    "Jane most similar to Christian and least similar to Ronnie\n",
    "Max most similar to Sean and least similar to Ronnie\n",
    "Minty most similar to Max and least similar to Ronnie\n",
    "Other most similar to Minty and least similar to Ronnie\n",
    "Phil most similar to Max and least similar to Ronnie\n",
    "Ronnie most similar to Max and least similar to Shirley\n",
    "Roxy most similar to Max and least similar to Shirley\n",
    "Sean most similar to Max and least similar to Ronnie\n",
    "Shirley most similar to Max and least similar to Ronnie\n",
    "Stacey most similar to Max and least similar to Ronnie\n",
    "Tanya most similar to Max and least similar to Ronnie"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Add dialogue context and scene features (20 marks)\n",
    "Adjust `create_character_document_from_dataframe` and the other functions appropriately so the data incorporates the context of the line spoken by the characters in terms of the lines spoken by other characters in the same scene (before and after the target character's lines). HINT: you should use the *Episode* and *Scene* columns to check which characters are in the same scene to decide whether to include their lines or not. You can also use **scene_info** column to extract information about the scene **(but DO USE the GENDER and CHARACTER columns directly)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Don't use this #####\n",
    "# # Create one document per character\n",
    "# def create_character_document_from_dataframe(df, max_line_count):\n",
    "#     \"\"\"Returns a dict with the name of the character as key,\n",
    "#     their lines joined together as a single string, with end of line _EOL_\n",
    "#     markers between them.\n",
    "    \n",
    "#     ::max_line_count:: the maximum number of lines to be added per character\n",
    "#     \"\"\"\n",
    "#     character_docs = {}\n",
    "#     character_line_count = {}\n",
    "#     for line, name, scene in zip(range(len(df.Line)), df.Character_name, df.episode_scene):\n",
    "#         if not name in character_docs.keys():\n",
    "#             character_docs[name] = \"\"\n",
    "#             character_line_count[name] = 0\n",
    "#         if character_line_count[name]==max_line_count:\n",
    "#             continue\n",
    "#         character_docs[name] += f'{str(df.Line.iloc[line])}'  + \" _EOL_ \"  # adding an end-of-line token\n",
    "#         character_line_count[name]+=1\n",
    "\n",
    "#         character_docs[name] += str(df.Scene_info.iloc[line]) + \" \"\n",
    "\n",
    "#         character_docs[name] += str(df.episode_scene.iloc[line]) + \" _SOL_ \"  # adding an end-of-line token\n",
    "#     print(\"lines per character\", character_line_count)\n",
    "#     return character_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mine\n",
    "# # Create one document per character\n",
    "# def create_character_document_from_dataframe(df, max_line_count):\n",
    "#     \"\"\"Returns a dict with the name of the character as key,\n",
    "#     their lines joined together as a single string, with end of line _EOL_\n",
    "#     markers between them.\n",
    "    \n",
    "#     ::max_line_count:: the maximum number of lines to be added per character\n",
    "#     \"\"\"\n",
    "#     character_docs = {}\n",
    "#     character_line_count = {}\n",
    "#     for line, name, episode_scenes in zip(df.Line, df.Character_name, df.episode_scene):\n",
    "#         if not name in character_docs.keys():\n",
    "#             character_docs[name] = \"\"\n",
    "#             character_line_count[name] = 0\n",
    "#         if character_line_count[name]==max_line_count:\n",
    "#             continue\n",
    "#         character_docs[name] += str(line)  + \" _EOL_ \"  # adding an end-of-line token\n",
    "#         character_docs[name] += str(episode_scenes) + \" _SOL_ \"  # adding an end-of-line token\n",
    "#         character_line_count[name]+=1\n",
    "#     print(\"lines per character\", character_line_count)\n",
    "#     return character_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best so far\n",
    "# Create one document per character\n",
    "def create_character_document_from_dataframe(df, max_line_count):\n",
    "    \"\"\"Returns a dict with the name of the character as key,\n",
    "    their lines joined together as a single string, with end of line _EOL_\n",
    "    markers between them.\n",
    "    \n",
    "    ::max_line_count:: the maximum number of lines to be added per character\n",
    "    \"\"\"\n",
    "    character_docs = {}\n",
    "    character_line_count = {}\n",
    "    for line, name, episode, scene in zip(range(len(df.Line)), df.Character_name, df.Episode, df.Scene):\n",
    "        if not name in character_docs.keys():\n",
    "            character_docs[name] = \"\"\n",
    "            character_line_count[name] = 0\n",
    "        if character_line_count[name]==max_line_count:\n",
    "            continue\n",
    "        # for line in the same scene\n",
    "        if line > 0 and df.Episode.iloc[line-1] == episode and df.Scene.iloc[line-1] == scene:\n",
    "            character_docs[name] += str(df.Line.iloc[line-1]) + \"_EOL_ \"\n",
    "        character_docs[name] += str(df.Line.iloc[line])  + \" _EOL_ \"\n",
    "        if line < len(df.Line)-1 and df.Episode.iloc[line+1] == episode and df.Scene.iloc[line+1] == scene:\n",
    "            character_docs[name] += str(df.Line.iloc[line+1])  + \" _EOL_ \"\n",
    "        character_line_count[name]+=1\n",
    "    print(\"lines per character\", character_line_count)\n",
    "    return character_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Improve the vectorization method (20 marks)\n",
    "Use a matrix transformation technique like TF-IDF (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to improve the `create_document_matrix_from_corpus` function, which currently only uses a dictionary vectorizor (`DictVectorizer`) which straight-forwardly maps from the feature dictionaries produced for each character document to a sparse matrix.\n",
    "\n",
    "As the `create_document_matrix_from_corpus` is designed to be used both in training/fitting (with `fitting` set to `True`) and in transformation alone on test/validation data (with `fitting` set to `False`), make sure you initialize any transformers you want to try in the same place as `corpusVectorizer = DictVectorizer()` before you call \n",
    "`create_document_matrix_from_corpus`. Again, develop on 90% training 10% validation split and note the effect/improvement in mean rank with each technique you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
    "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
    "# tfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "\n",
    "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
    "    \"\"\"Method which fits different vectorizers\n",
    "    on data and returns a matrix.\n",
    "    \n",
    "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
    "    \n",
    "    ::corpus:: a list of (class_label, document) pairs.\n",
    "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
    "    \"\"\"\n",
    "    \n",
    "    # uses the global variable of the corpus Vectorizer to improve things\n",
    "    if fitting:\n",
    "        corpusVectorizer.fit([to_feature_vector_dictionary(doc, []) for name, doc in corpus])\n",
    "        tfidfTransformer.fit(corpusVectorizer.transform([to_feature_vector_dictionary(doc, []) for name, doc in corpus]))\n",
    "        \n",
    "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc, []) for name, doc in corpus])\n",
    "    doc_feature_matrix = tfidfTransformer.transform(doc_feature_matrix)\n",
    "\n",
    "    return doc_feature_matrix\n",
    "\n",
    "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
    "\n",
    "# mean rank: 1.25, accuracy: 0.875"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Run on final test data  (10 marks)\n",
    "Test your best system using the code below to train on all of the training data (using the first 400 lines per character maximum) and do the final testing on the test file (using the first 40 lines per character maximum).\n",
    "\n",
    "Make any neccessary adjustments such that it runs in the same way as the training/testing regime you developed above- e.g. making sure any transformer objects are initialized before `create_document_matrix_from_corpus` is called. Make sure your best system is left in the notebook and it is clear what the mean rank, accuracy of document selection are on the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86fb86a74c913095f2328121504a2cb38245fc0b392e5f183237bbf35130b558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
